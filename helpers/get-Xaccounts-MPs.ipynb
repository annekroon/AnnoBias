{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd526004-15a1-44a8-ad70-1884590a0b13",
   "metadata": {},
   "source": [
    "## GET TWITTER ACCOUNTS OF POLITICIANS\n",
    "\n",
    "\n",
    "The provided code can be used for scraping data from Tweedekamer.nl, Wikipedia and Wikidata related to members of the Dutch Tweede Kamer (House of Representatives). It includes functions to extract information about current members of the Tweede Kamer, members from specific periods (2012-2017, 2017-2021, 2021-2023, 2024), and to fetch additional data from Wikidata such as gender, date of birth, social media usernames, and more. The script uses the BeautifulSoup library to parse HTML content and the requests library to make HTTP requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "60d0915a-baf4-42e7-b615-d4b9a5fa67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_wiki_data(wikidata_id):\n",
    "    # URL for Wikidata API endpoint\n",
    "    wikidata_api_url = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"ids\": wikidata_id,\n",
    "        \"props\": \"claims\"  # Retrieve claims (properties) for the entity\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send GET request to Wikidata API\n",
    "        response = requests.get(url=wikidata_api_url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract desired properties from the response\n",
    "        properties = {}\n",
    "\n",
    "          # Extract gender\n",
    "        if \"P21\" in data[\"entities\"][wikidata_id][\"claims\"]:\n",
    "            gender_qid = data[\"entities\"][wikidata_id][\"claims\"][\"P21\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "            gender = map_gender(gender_qid)\n",
    "            properties[\"gender\"] = gender\n",
    "\n",
    "        # Extract date of birth\n",
    "        if \"P569\" in data[\"entities\"][wikidata_id][\"claims\"]:\n",
    "            bday = data[\"entities\"][wikidata_id][\"claims\"][\"P569\"][0][\"mainsnak\"][\"datavalue\"]['value']['time']\n",
    "           # bday = pd.to_datetime\n",
    "            properties[\"birthday\"] = bday\n",
    "\n",
    "        # Extract social media usernames\n",
    "        if \"P2002\" in data[\"entities\"][wikidata_id][\"claims\"]:\n",
    "            twitter_username = data[\"entities\"][wikidata_id][\"claims\"][\"P2002\"][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "            properties[\"twitter_username\"] = twitter_username\n",
    "\n",
    "        # Extract Twitter numeric user ID\n",
    "        if \"P2002\" in data[\"entities\"][wikidata_id][\"claims\"]:\n",
    "            twitter_id = data[\"entities\"][wikidata_id][\"claims\"][\"P2002\"][0][\"mainsnak\"][\"datavalue\"][\"value\"]\n",
    "            properties[\"twitter_numeric_user_id\"] = twitter_id\n",
    "\n",
    "        # Extract subscriber count\n",
    "        if \"P2002\" in data[\"entities\"][wikidata_id][\"claims\"]:\n",
    "            qualifiers = data[\"entities\"][wikidata_id][\"claims\"][\"P2002\"][0].get(\"qualifiers\", {})\n",
    "            if \"P3744\" in qualifiers:\n",
    "                subscriber_count = int(qualifiers[\"P3744\"][0][\"datavalue\"][\"value\"][\"amount\"])\n",
    "                properties[\"subscriber_count\"] = subscriber_count\n",
    "\n",
    "        # Extract start date\n",
    "        if \"P2002\" in data[\"entities\"][wikidata_id][\"claims\"]:\n",
    "            qualifiers = data[\"entities\"][wikidata_id][\"claims\"][\"P2002\"][0].get(\"qualifiers\", {})\n",
    "            if \"P580\" in qualifiers:\n",
    "                start_date_str = qualifiers[\"P580\"][0][\"datavalue\"][\"value\"][\"time\"]\n",
    "               # start_date = pd.to_datetime(start_date_str[1:])\n",
    "                properties[\"start_date\"] = start_date_str\n",
    "\n",
    "        # Extract point in time\n",
    "        if \"P2002\" in data[\"entities\"][wikidata_id][\"claims\"]:\n",
    "            qualifiers = data[\"entities\"][wikidata_id][\"claims\"][\"P2002\"][0].get(\"qualifiers\", {})\n",
    "            if \"P585\" in qualifiers:\n",
    "                point_in_time_str = qualifiers[\"P585\"][0][\"datavalue\"][\"value\"][\"time\"]\n",
    "                #point_in_time = pd.to_datetime(point_in_time_str[1:])\n",
    "                properties[\"point_in_time\"] = point_in_time_str\n",
    "\n",
    "        return properties\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching data from Wikidata API:\", e)\n",
    "        return None\n",
    "\n",
    "def map_gender(qid):\n",
    "    if qid == \"Q6581072\":\n",
    "        return \"female\"\n",
    "    elif qid == \"Q6581097\":\n",
    "        return \"male\"\n",
    "    else:\n",
    "        return \"other/unknown\"\n",
    "        \n",
    "def get_wikidata_id(name):\n",
    "    # URL for Wikidata API endpoint\n",
    "    wikidata_api_url = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"type\": \"item\",\n",
    "        \"search\": name\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send GET request to Wikidata API\n",
    "        response = requests.get(url=wikidata_api_url, params=params)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract Wikidata ID from response\n",
    "        if data[\"search\"]:\n",
    "            wikidata_id = data[\"search\"][0][\"id\"]\n",
    "            return wikidata_id\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except: \n",
    "        print(\"Error fetching wiki id:\", e)\n",
    "        return None\n",
    "        \n",
    "\n",
    "def scrape_tweedekamer(url):\n",
    "\n",
    "    ''' This function gets all current Tweede Kamerleden as registered on tweedekamer.nl'''\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Find all member cards\n",
    "    member_cards = soup.find_all(\"div\", class_=\"u-member-card-height\")\n",
    "\n",
    "    # List to store information for all members\n",
    "    members_info = []\n",
    "\n",
    "    # Iterate through each member card\n",
    "    for card in member_cards:\n",
    "        # Extract name and party\n",
    "        name = card.find(\"a\", class_=\"u-text-size--large\").text.strip()\n",
    "        party = card.find(\"span\", class_=\"u-text-size--small\").text.strip()\n",
    "\n",
    "        wikidata_id = get_wikidata_id(name)\n",
    "\n",
    "        # Extract image URL if it exists\n",
    "        image_tag = card.find(\"img\", class_=\"m-avatar__image\")\n",
    "        image_url = image_tag[\"src\"] if image_tag else None\n",
    "\n",
    "        # Extract additional details from the table\n",
    "        table_rows = card.find(\"table\", class_=\"u-text-size--small\").find_all(\"tr\")\n",
    "\n",
    "        # Initialize variables to store location, age, and seniority\n",
    "        location = \"\"\n",
    "        age = \"\"\n",
    "        seniority = \"\"\n",
    "\n",
    "        # Iterate through each table row and extract information\n",
    "        for row in table_rows:\n",
    "            header = row.find(\"th\").text.strip()\n",
    "            value = row.find(\"td\").text.strip()\n",
    "            if \"Woonplaats\" in header:\n",
    "                location = value\n",
    "            elif \"Leeftijd\" in header:\n",
    "                age = value\n",
    "            elif \"AnciÃ«nniteit\" in header:\n",
    "                seniority = value\n",
    "        # Store the information in a dictionary\n",
    "        member_info = {\n",
    "            \"Name\": name,\n",
    "            \"Party\": party,\n",
    "            \"Location\": location,\n",
    "            \"Age\": age,\n",
    "            \"Seniority\": seniority,\n",
    "            \"Image_URL\": image_url,\n",
    "            \"Wikidata_ID\": wikidata_id\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        members_info.append(member_info)\n",
    "\n",
    "    # Get Wikidata properties for each member\n",
    "    for member in members_info:\n",
    "        # Get Wikidata properties only if Wikidata ID is available\n",
    "        if member[\"Wikidata_ID\"]:\n",
    "            wiki_data = get_wiki_data(member[\"Wikidata_ID\"])\n",
    "            # Merge Wikidata properties into member info dictionary\n",
    "            member.update(wiki_data)\n",
    "            \n",
    "    df = pd.DataFrame(members_info)\n",
    "    return df\n",
    "\n",
    "def scrape_politicians(url):\n",
    "    \n",
    "    ''' This function gets all Tweede Kamerleden in the period 2017-2013 and 2012-2017 from wikipedia'''\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Create a BeautifulSoup object\n",
    "    \n",
    "    rows = soup.find_all('tr')\n",
    "    \n",
    "    \n",
    "    # Initialize a list to store the scraped data\n",
    "    scraped_data = []\n",
    "    \n",
    "    # Loop through each row and extract the desired information\n",
    "    for row in rows:\n",
    "        # Find all table data (td) within the row\n",
    "        cells = row.find_all('td')\n",
    "        \n",
    "         # Extract information from cells if they exist and the length is as expected\n",
    "        if len(cells) >= 4:\n",
    "            try:\n",
    "                name = row.find('a').text.strip()  # Extract the name\n",
    "            except AttributeError:\n",
    "                continue  # Skip this row if the 'a' tag is not found\n",
    "            party = cells[1].text.strip()      # Extract the party\n",
    "            \n",
    "            # Use try-except block to handle potential IndexError\n",
    "            try:\n",
    "                start_date = cells[2].text.strip() # Extract the start date\n",
    "                end_date = cells[3].text.strip()   # Extract the end date\n",
    "            except IndexError:\n",
    "                start_date = \"N/A\"\n",
    "                end_date = \"N/A\"\n",
    "            \n",
    "            wiki_id = get_wikidata_id(name)\n",
    "            if wiki_id is not None: \n",
    "                wikidata = get_wiki_data(wiki_id)\n",
    "            \n",
    "            # Append the extracted information to the scraped_data list\n",
    "            scraped_data.append({\n",
    "                'Name': name,\n",
    "                'Party': party,\n",
    "                'Start Date': start_date,\n",
    "                'End Date': end_date, \n",
    "                'Wikidata_ID' : wikidata_id , \n",
    "                **wikidata\n",
    "            })\n",
    "        \n",
    "    return scraped_data\n",
    "\n",
    "\n",
    "def scrape_20172021(url):\n",
    "        \n",
    "    ''' This function gets all Tweede Kamerleden in the period 2017-2021 from wikipedia'''\n",
    "\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    scraped_data = []\n",
    "    # Find all sections containing party information\n",
    "    party_sections = soup.find_all('span', class_='mw-headline')\n",
    "    \n",
    "    for party_section in party_sections:\n",
    "        party_name = party_section.text.strip()\n",
    "        \n",
    "        # Replace \"Samenstelling\" with \"VVD\"\n",
    "        if party_name == \"Samenstelling\":\n",
    "            party_name = \"VVD\"\n",
    "        \n",
    "        # Find the table within the party section\n",
    "        table = party_section.find_next('table', class_='wikitable sortable')\n",
    "        if table:\n",
    "            # Extract data from the table\n",
    "            rows = table.find_all('tr')\n",
    "            current_name = None  # To track the name being processed\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                if columns:\n",
    "                    name = columns[0].find('a')\n",
    "                    if name:\n",
    "                        current_name = name.text.strip()\n",
    "                    else:\n",
    "                        current_name = \"N/A\"\n",
    "                    \n",
    "                    # Check if the column index exists before accessing it\n",
    "                    if len(columns) > 1:\n",
    "                        start_date = columns[1].text.strip()\n",
    "                    else:\n",
    "                        start_date = \"N/A\"\n",
    "                    \n",
    "                    if len(columns) > 2:\n",
    "                        end_date = columns[2].text.strip()\n",
    "                    else:\n",
    "                        end_date = \"N/A\"\n",
    "                        \n",
    "                    wiki_id = get_wikidata_id(current_name)\n",
    "                    \n",
    "                    if wiki_id is not None: \n",
    "                        wikidata = get_wiki_data(wiki_id)\n",
    "                    \n",
    "                    # Append the extracted information to the scraped_data list\n",
    "                    scraped_data.append({\n",
    "                        'Name': current_name,\n",
    "                        'Party': party_name,\n",
    "                        'Start Date': start_date,\n",
    "                        'End Date': end_date, \n",
    "                        'Wikidata_ID' : wikidata_id , \n",
    "                        **wikidata\n",
    "                    })\n",
    "        \n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f57e5-fbf7-4e61-8d5d-17a3cd494f21",
   "metadata": {},
   "source": [
    "## Call functions and create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "416ca03c-a620-4d4c-844b-555b2b7f4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2012 -- 2017\n",
    "\n",
    "wiki_url_2012= 'https://nl.wikipedia.org/wiki/Lijst_van_Tweede_Kamerleden_2012-2017'\n",
    "tweede_kamerleden2012_2017 = pd.DataFrame(scrape_politicians(wiki_url_2012))\n",
    "tweede_kamerleden2012_2017['period'] = \"2012_2017\"\n",
    "## 2017 -- 2021\n",
    "\n",
    "wiki_url_2017 = 'https://nl.wikipedia.org/wiki/Lijst_van_Tweede_Kamerleden_2017-2021'\n",
    "tweede_kamerleden2017_2021 = pd.DataFrame(scrape_20172021(wiki_url_2017))\n",
    "tweede_kamerleden2017_2021['period'] = \"2017_2021\"\n",
    "\n",
    "## 2021 -- 2023\n",
    "\n",
    "wiki_url = \"https://nl.wikipedia.org/w/index.php?title=Lijst_van_Tweede_Kamerleden_2021-2023\"\n",
    "tweede_kamerleden2021_2023 = pd.DataFrame(scrape_politicians(wiki_url))\n",
    "tweede_kamerleden2021_2023['period'] = \"2021_2023\"\n",
    "\n",
    "## 2024 (current)\n",
    "\n",
    "tk_url = \"https://www.tweedekamer.nl/kamerleden_en_commissies/alle_kamerleden\"\n",
    "tweede_kamerledencurrent= scrape_tweedekamer(tk_url)\n",
    "tweede_kamerledencurrent['period'] = '2024'\n",
    "\n",
    "combined_data = pd.concat([tweede_kamerleden2012_2017, tweede_kamerleden2017_2021, tweede_kamerleden2021_2023, tweede_kamerledencurrent], axis=0) \n",
    "combined_data.to_csv('../data/tweedekamerleden.csv')\n",
    "\n",
    "# Filter out rows where 'twitter_username' is not NaN\n",
    "non_null_twitter_usernames = combined_data[combined_data['twitter_username'].notna()]\n",
    "\n",
    "print(len(non_null_twitter_usernames))\n",
    "# Extract unique Twitter usernames\n",
    "unique_usernames = list(set(non_null_twitter_usernames['twitter_username']))\n",
    "\n",
    "print(len(unique_usernames))\n",
    "# Define the path for the output text file\n",
    "output_file_path = \"../data/MPs_twitter_usernames.txt\"\n",
    "\n",
    "# Write the unique Twitter usernames to the text file\n",
    "with open(output_file_path, \"w\") as file:\n",
    "    for username in unique_usernames:\n",
    "        file.write(username + \"\\n\")\n",
    "\n",
    "print(\"Unique Twitter usernames have been written to:\", output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
